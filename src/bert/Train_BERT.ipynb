{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35aa5ee2",
   "metadata": {},
   "source": [
    "# Train the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3a394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n",
    "path = os.path.realpath(\"../data/dataset_fen_small.txt\")\n",
    "\n",
    "tokenizer_folder = os.path.realpath(\"./bert-harmon\")\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "vocab_size = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a067bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/michael/Workspace/nlp-chess/src/bert/bert-harmon/vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=path, vocab_size=vocab_size, min_frequency=1,\n",
    "                show_progress=True,\n",
    "                special_tokens=[ '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', \"[MOVESEP]\"])\n",
    "#Save the Tokenizer to disk\n",
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea871f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/michael/Workspace/nlp-chess/src/bert/bert-harmon/vocab.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d27dd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'rnbqkbnr',\n",
       " '/',\n",
       " 'pppppppp',\n",
       " '/',\n",
       " '8',\n",
       " '/',\n",
       " '8',\n",
       " '/',\n",
       " '8',\n",
       " '/',\n",
       " '8',\n",
       " '/',\n",
       " 'pppppppp',\n",
       " '/',\n",
       " 'rnbqkbnr',\n",
       " 'w',\n",
       " 'kq',\n",
       " '##k',\n",
       " '##q',\n",
       " '-',\n",
       " '0',\n",
       " '1',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'e2e4',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(os.path.abspath(os.path.join(tokenizer_folder, 'vocab.txt')))\n",
    "# Prepare the tokenizer\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "tokenizer.enable_truncation(max_length=128)\n",
    "# Test the tokenizer\n",
    "tokenizer.encode(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1 <MOVE_SEP> e2e4\")\n",
    "# Show the tokens created\n",
    "tokenizer.encode(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1 <MOVE_SEP> e2e4\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8964f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  56115712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Workspace/nlp-chess/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1648: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = BertForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())\n",
    "\n",
    "# Create the tokenizer from a trained one\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_folder + \"/vocab.txt\", max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5161cf5",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb44923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-20e42cf60b461c77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/michael/.cache/huggingface/datasets/text/default-20e42cf60b461c77/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebae1e2c2004a368f73ac3b8f65d9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1050750d1b407bb364559a99767bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/michael/.cache/huggingface/datasets/text/default-20e42cf60b461c77/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import set_caching_enabled\n",
    "raw_datasets = load_dataset('text', data_files=path,\n",
    "                            split='train')\n",
    "\n",
    "#cut size in half\n",
    "raw_datasets = raw_datasets.shuffle(seed=42).select(range(int(len(raw_datasets) / 2.5)))\n",
    "\n",
    "#raw_datasets = raw_datasets.select(range(10000))\n",
    "raw_datasets = raw_datasets.train_test_split()\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, keep_in_memory=True, num_proc=20, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9fdc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 7795, 63, 54, 7, 8811, 7, 192, 8523, 7, 444, 7, 705, 7, 3065, 7, 1787, 7, 1295, 32, 6, 6, 8, 339, 29, 1, 30, 2007, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "grouping complete\n"
     ]
    }
   ],
   "source": [
    "# tokenized_datasets = raw_datasets\n",
    "#     with open(\"dataset-tokenized.obj\", 'wb') as f:\n",
    "#         pickle.dump(tokenized_datasets, f)\n",
    "\n",
    "# block_size = tokenizer.model_max_length\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    #result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "print(tokenized_datasets[\"train\"][1])\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=20,\n",
    "    keep_in_memory=True\n",
    ")\n",
    "\n",
    "print(\"grouping complete\")\n",
    "\n",
    "small_train_dataset = lm_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_train_dataset[\"labels\"] = small_train_dataset[\"input_ids\"]\n",
    "small_eval_dataset = lm_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset[\"labels\"] = small_eval_dataset[\"input_ids\"]\n",
    "full_train_dataset = lm_datasets[\"train\"]\n",
    "full_eval_dataset = lm_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c5541c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3194"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 3194\n",
      "  Num Epochs = 2000\n",
      "  Instantaneous batch size per device = 392\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 392\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='18000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  102/18000 01:03 < 3:09:11, 1.58 it/s, Epoch 11.22/2000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=392,\n",
    "    output_dir='./output', \n",
    "    num_train_epochs=2000,)\n",
    "\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=full_train_dataset, eval_dataset=full_eval_dataset, data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2f3903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Workspace/transformers/examples/pytorch/language-modeling\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeaf8e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13857084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ee7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
